
import numpy as np
import scipy as sp
import math
import pandas as pd 
import time
import geweke
import os
import gc
import sys
from scipy.sparse import csc_matrix
from numba import njit

import utility_functions as uf

@njit(cache=True, fastmath=True)
def sample_gamma_numba(beta, sigma_0, sigma_1, pi0, gamma):
	inv_s0 = 1.0 / sigma_0
	inv_s1 = 1.0 / sigma_1
	
	c0 = inv_s0 / math.sqrt(2.0 * math.pi)
	c1 = inv_s1 / math.sqrt(2.0 * math.pi)

	a0 = 0.5 * inv_s0 * inv_s0
	a1 = 0.5 * inv_s1 * inv_s1
	
	for i in range(beta.size):
		b = beta[i]
		# densities
		d0 = (1.0 - pi0) * c0 * math.exp(-b * b * a0)
		d1 = pi0* c1 * math.exp(-b * b * a1)

		post = d1 / (d0 + d1)    # posterior P(gamma=1 | beta)
		gamma[i] = 1 if np.random.random() < post else 0

	return(gamma)


def sample_gamma(beta,sigma_0,sigma_1,pie):
	p = np.empty(len(beta))
	d1 = pie*sp.stats.norm.pdf(beta,loc=0,scale=sigma_1)
	d0 = (1-pie)*sp.stats.norm.pdf(beta,loc=0,scale=sigma_0)
	p = d1/(d0+d1)
	gamma = np.random.binomial(1,p).astype(np.int64)
	return(gamma)

def sample_pie(gamma,pie_a,pie_b):
	a_new = np.sum(gamma)+pie_a
	b_new = np.sum(1-gamma)+pie_b
	pie_new = np.random.beta(a_new,b_new)
	return(pie_new)

def sample_sigma_1(beta,gamma,a_sigma,b_sigma):
	a_new = 0.5*np.sum(gamma)+a_sigma
	b_new = 0.5*np.sum(np.multiply(np.square(beta),gamma))+b_sigma
	sigma_1_neg2 =np.random.gamma(a_new,1.0/b_new)
	sigma_1_new = math.sqrt(1/sigma_1_neg2)
	return(sigma_1_new)

def sample_sigma_e(y,H_beta,C_alpha,a_e,b_e):
	n = len(y)
	a_new = float(n)/2+a_e
	resid = y - H_beta - C_alpha
	b_new = np.sum(np.square(resid))/2+b_e
	sigma_e_neg2 =np.random.gamma(a_new,1.0/b_new)
	sigma_e_new = math.sqrt(1/sigma_e_neg2)
	return(sigma_e_new)

def sample_alpha(y,H_beta,C_alpha,C,alpha,sigma_e,C_norm_2):

	r,c = C.shape

	if c == 1:
		#new_variance = 1/(np.linalg.norm(C[:,0])**2*sigma_e**-2)
		new_variance = 1/(C_norm_2[0]*sigma_e**-2)
		new_mean = new_variance*np.dot((y-H_beta),C[:,0])*sigma_e**-2
		alpha = np.random.normal(new_mean,math.sqrt(new_variance))
		C_alpha = C[:,0] * alpha
	else:
		for i in range(c):
			#new_variance = 1/(np.linalg.norm(C[:,i])**2*sigma_e**-2)
			new_variance = 1/(C_norm_2[i]*sigma_e**-2)
			C_alpha_negi = C_alpha - C[:,i] * alpha[i]
			new_mean = new_variance*np.dot(y-C_alpha_negi-H_beta,C[:,i])*sigma_e**-2
			alpha[i] = np.random.normal(new_mean,math.sqrt(new_variance))
			C_alpha = C_alpha_negi + C[:,i] * alpha[i]

	return(alpha,C_alpha)


def sample_beta(y,C_alpha,H_beta,H,beta,gamma,sigma_0,sigma_1,sigma_e,H_norm_2):

	sigma_e_neg2 = sigma_e**-2
	sigma_0_neg2 = sigma_0**-2
	sigma_1_neg2 = sigma_1**-2

	for i in range(len(beta)):

		H_beta_negi = H_beta - H[:,i] * beta[i] 	### original


		residual = y - C_alpha -  H_beta + H[:,i] * beta[i]		### original

		new_variance = 1/(H_norm_2[i]*sigma_e_neg2+(1-gamma[i])*sigma_0_neg2+gamma[i]*sigma_1_neg2)		### original


		new_mean = new_variance*np.dot(residual,H[:,i])*sigma_e_neg2		### original

		beta[i] = np.random.normal(new_mean,math.sqrt(new_variance))		### original

		H_beta = H_beta_negi + H[:,i] * beta[i]		### original


	return(beta,H_beta)



@njit
def sample_beta_numba(y, C_alpha, H_beta, H, beta, gamma, sigma_0, sigma_1, sigma_e, H_norm_2):
	sigma_e_neg2 = sigma_e ** -2
	sigma_0_neg2 = sigma_0 ** -2
	sigma_1_neg2 = sigma_1 ** -2
	ncols = beta.shape[0]
	nrows = y.shape[0]
    
	for i in range(ncols):

		for r in range(nrows):
			H_beta[r] -= H[r, i] * beta[i]

        # Compute the dot product over the column using the updated H_beta.
		dot_val = 0.0
		for r in range(nrows):
            # residual = y[r] - C_alpha[r] - H_beta[r]
			res_val = y[r] - C_alpha[r] - H_beta[r] 
			dot_val += res_val * H[r, i]
        
		new_variance = 1.0 / (H_norm_2[i]*sigma_e_neg2 + (1 - gamma[i])*sigma_0_neg2 + gamma[i]*sigma_1_neg2)
		new_mean = new_variance * sigma_e_neg2 * dot_val
        
        # Sample new beta using standard normal (Numba supports np.random.randn)
		beta[i] = new_mean + math.sqrt(new_variance) * np.random.randn()
       
        # Update H_beta with the new contribution.
		for r in range(nrows):
			H_beta[r] += H[r, i] * beta[i]
    
	return (beta, H_beta)



def sample_beta_sparse(y, C_alpha, H_beta, H, beta, gamma, sigma_0, sigma_1, sigma_e, H_norm_2):

	# Precompute inverse squares for efficiency.
	sigma_e_neg2 = sigma_e ** -2
	sigma_0_neg2 = sigma_0 ** -2
	sigma_1_neg2 = sigma_1 ** -2

    # Ensure H is in CSC format for fast column slicing.
	H_sparse = csc_matrix(H)

    # Loop over each column (i.e. each beta element)
	for i in range(len(beta)):
		col = H_sparse.getcol(i)
		indices = col.indices
		data = col.data

        # Remove the old contribution of column i from H_beta.
        # This only touches the nonzero indices in column i.
		H_beta[indices] -= data * beta[i]

		residual = y - C_alpha - H_beta

        # Compute new variance and new mean using the nonzero entries only.
		new_variance = 1 / (H_norm_2[i] * sigma_e_neg2 + (1 - gamma[i]) * sigma_0_neg2 + gamma[i] * sigma_1_neg2)
        # Dot product over nonzero entries: sum(residual[indices] * data)
		new_mean = new_variance * sigma_e_neg2 * np.dot(residual[indices], data)

		np.random.seed(i)

        # Sample new beta value from a normal distribution.
		beta[i] = np.random.normal(new_mean, math.sqrt(new_variance))

        # Add the new contribution of column i back to H_beta.
		H_beta[indices] += data * beta[i]

	return (beta, H_beta)


def sampling(verbose,y,C,HapDM,sig0_initiate,prefix,num,trace_container,gamma_container,beta_container,alpha_container,convergence_container,pi_b):

	## set random seed for the process
	np.random.seed(int(time.time()) + os.getpid())

	#initiate beta,gamma and H matrix
	H = np.array(HapDM)

	H_r,H_c = H.shape
	C_c = C.shape[1]

	##specify hyper parameters
	pie_a = 1
	pie_b = H_c * pi_b
	a_sigma = 1
	b_sigma = 1
	a_e = 1
	b_e = 1
	
	H_var = np.sum(np.var(H,axis=0))
	sigma_0 = np.sqrt(np.var(y) / H_var * sig0_initiate)
	sigma_1 = math.sqrt(1/np.random.gamma(a_sigma,b_sigma))
	sigma_e = math.sqrt(1/np.random.gamma(a_e,b_e))
	pie = np.random.beta(pie_a,pie_b)

	if verbose > 0:
		print("There are %i k-mers in the model, and to set the background variation %f of the total phenotypic variation.\n We set the sigma 0 to be %f" %(H_c,sig0_initiate,sigma_0) )

		print("initiation for chain %i:" %(num) ,sigma_1,sigma_e,pie)
		
	it = 0
	burn_in_iter = 2000
	step_size = 2000


	convergence_start_iter = burn_in_iter
	convergence_end_iter = np.array(range(convergence_start_iter*2,convergence_start_iter+step_size*4,step_size))

	convergence_iter = convergence_start_iter+step_size*3

	trace = np.empty((convergence_end_iter[-1]-burn_in_iter,7))
	top5_beta_trace = np.empty((convergence_end_iter[-1]-burn_in_iter,5))

	alpha = np.random.random(size = C_c)
	gamma = np.random.binomial(1,pie,H_c)
	beta = np.array(np.zeros(H_c))


	for i in range(H_c):
		if gamma[i] == 0:
			beta[i] = np.random.normal(0,sigma_0)
		else:
			beta[i] = np.random.normal(0,sigma_1) 

	#start sampling

	H_beta = np.matmul(H,beta)
	C_alpha = np.matmul(C,alpha)

	## precompute some variables 

	C_norm_2 = np.sum(C**2,axis=0)
	H_norm_2 = np.sum(H**2,axis=0)


	while it < convergence_iter:
		before = time.time()

		sigma_1 = sample_sigma_1(beta,gamma,a_sigma,b_sigma)
		if sigma_1 < sigma_0 * 5:
			sigma_1 = sigma_0 * 5
			pie = 0
		else:
			pie = sample_pie(gamma,pie_a,pie_b)
		sigma_e = sample_sigma_e(y,H_beta,C_alpha,a_e,b_e)
		gamma = sample_gamma(beta,sigma_0,sigma_1,pie)
		alpha,C_alpha = sample_alpha(y,H_beta,C_alpha,C,alpha,sigma_e,C_norm_2)
		beta,H_beta = sample_beta_numba(y,C_alpha,H_beta,H,beta,gamma,sigma_0,sigma_1,sigma_e,H_norm_2)
		genetic_var = np.var(H_beta)
		pheno_var = np.var(y - C_alpha)
		large_beta = np.absolute(beta) > 0.3
		large_beta_ratio = np.sum(large_beta) / len(beta)
		total_heritability = genetic_var / pheno_var
		alpha_norm = np.linalg.norm(alpha, ord=2)
		beta_norm = np.linalg.norm(beta, ord=2)

		after = time.time()
		if (it > 2000 and total_heritability > 1) or (it > 2000 and sum(gamma)<0):
			if verbose > 0:
				print("unrealistic beta sample",it,genetic_var,pheno_var,total_heritability)
			continue

		else:
			if verbose > 1:
				print(num,it,str(after - before),sigma_1,sigma_e,large_beta_ratio,total_heritability,sum(gamma))

			if it >= burn_in_iter:
				trace[it-burn_in_iter,:] = [alpha_norm,beta_norm,sigma_1,sigma_e,large_beta_ratio,total_heritability,sum(gamma)]
				top5_beta_trace[it-burn_in_iter,:] = np.sort(np.absolute(beta))[::-1][:5]

			if it == convergence_end_iter[-1] - 1:
				
				num_convergence_test = len(convergence_end_iter)

				convergence_scores = np.zeros(len(convergence_end_iter))

				for s in range(num_convergence_test):
					convergence_scores[s] = uf.convergence_geweke_test(trace,top5_beta_trace,convergence_start_iter-burn_in_iter,convergence_end_iter[s]-burn_in_iter)

				if np.sum(convergence_scores) == num_convergence_test:
					convergence_container[num] = 1

					if verbose > 0:
						print("convergence has been reached at %i iterations for chain %i. The MCMC Chain has entered a stationary stage" %(it,num))
						print("trace values:", trace[it-burn_in_iter,:])
					break
				else:
					trace_ = np.empty((1000,7))
					top5_beta_trace_ = np.empty((1000,5))


					trace = np.concatenate((trace[-(convergence_iter - burn_in_iter-1000):,:],trace_),axis=0)
					top5_beta_trace = np.concatenate((top5_beta_trace[-(convergence_iter - burn_in_iter-1000):,:],top5_beta_trace_),axis = 0)

					burn_in_iter += 1000
					convergence_iter += 1000

					convergence_start_iter += 1000
					convergence_end_iter += 1000

					#print(it,burn_in_iter,convergence_iter,convergence_start_iter,convergence_end_iter,trace.shape)

			it += 1

			if it > 100000: 
				convergence_container[num] = 0
				break

	if convergence_container[num] == 1:

		## MCMC draws for posterior mean

		posterior_draws = 10000

		alpha_mean = np.zeros(C_c)
		beta_mean = np.zeros(H_c)
		gamma_sum = np.zeros(H_c)

		alpha_M2 = np.zeros(C_c)
		beta_M2 = np.zeros(H_c)

		posterior_trace = np.empty((posterior_draws,7))

		alpha_trace = np.empty((posterior_draws,C_c))

		it = 0

		while it < posterior_draws:
		
			before = time.time()
			sigma_1 = sample_sigma_1(beta,gamma,a_sigma,b_sigma)
			if sigma_1 < sigma_0 * 5:
				sigma_1 = sigma_0 * 5
				pie = 0
			else:
				pie = sample_pie(gamma,pie_a,pie_b)
			sigma_e = sample_sigma_e(y,H_beta,C_alpha,a_e,b_e)
			gamma = sample_gamma(beta,sigma_0,sigma_1,pie)
			alpha,C_alpha = sample_alpha(y,H_beta,C_alpha,C,alpha,sigma_e,C_norm_2)
			beta,H_beta = sample_beta_numba(y,C_alpha,H_beta,H,beta,gamma,sigma_0,sigma_1,sigma_e,H_norm_2)
			genetic_var = np.var(H_beta)
			pheno_var = np.var(y - C_alpha)
			large_beta = np.absolute(beta) > 0.3
			large_beta_ratio = np.sum(large_beta) / len(beta)
			total_heritability = genetic_var / pheno_var
			alpha_norm = np.linalg.norm(alpha, ord=2)
			beta_norm = np.linalg.norm(beta, ord=2)
			after = time.time()
			if total_heritability > 1:
				if verbose > 0:
					print("unrealistic beta sample",it,genetic_var,pheno_var,total_heritability)
				continue

			else:
				if verbose >1 :
					print(it,str(after - before),pie,sigma_1,sigma_e,sum(gamma),large_beta_ratio,max(abs(beta)),total_heritability)

				posterior_trace[it,:] = [alpha_norm,beta_norm,sigma_1,sigma_e,large_beta_ratio,total_heritability,sum(gamma)]
				alpha_trace[it,:] = alpha
				beta_mean,beta_M2 = uf.welford(beta_mean,beta_M2,beta,it)
				alpha_mean,alpha_M2 = uf.welford(alpha_mean,alpha_M2,alpha,it)
				gamma_sum += gamma

				if verbose > 0:
					if it > 0 and it % 2000 == 0:
						print("Posterior draws: %i iterations have been sampled for chain %i" %(it,num), str(after - before),posterior_trace[it,:])
				it += 1

		trace_container[num] = posterior_trace

		#alpha values
		alpha_container[num] = {'avg': alpha_mean,
								'M2': alpha_M2}

		#beta values
		beta_container[num] = {'avg':beta_mean,
								'M2':beta_M2}

		gamma_container[num] = gamma_sum / posterior_draws

	else:
		trace_container[num] = []

		#alpha values
		alpha_container[num] = {'avg': [],
								'M2': []}

		#beta values
		beta_container[num] = {'avg':[],
								'M2':[]}

		gamma_container[num] = []

